{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# # ================================\n# # 1. LOAD DATA\n# # ================================\n# train = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\n# test  = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n\n# # ================================\n# # 2. BASIC EDA\n# # ================================\n# print(train.head())\n# print(train.info())\n# print(train.isnull().sum())\n# print(train['Status'].value_counts())\n\n# # TARGET & FEATURES\n# y = train[\"Status\"]\n# X = train.drop(\"Status\", axis=1)\n\n# num_cols = X.select_dtypes(include=['int64','float64']).columns\n# cat_cols = X.select_dtypes(include=['object']).columns\n\n\n\n# # ================================\n# # 4. HANDLE MISSING VALUES\n# # ================================\n# X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n# test[num_cols] = test[num_cols].fillna(test[num_cols].median())\n\n# X[cat_cols] = X[cat_cols].fillna(X[cat_cols].mode().iloc[0])\n# test[cat_cols] = test[cat_cols].fillna(test[cat_cols].mode().iloc[0])\n\n# X = X.reset_index(drop=True)\n# test = test.reset_index(drop=True)\n\n# # ================================\n# # 5. LABEL ENCODE TARGET\n# # ================================\n# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# y_encoded = le.fit_transform(y)\n\n# print(\"Label Mapping:\", le.classes_)  \n# # MUST be ['C', 'CL', 'D']\n# # ================================\n# # 6. PREPROCESS + MODEL\n# # ================================\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.ensemble import RandomForestClassifier\n\n# preprocess = ColumnTransformer(\n#     transformers=[\n#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n#         (\"num\", \"passthrough\", num_cols)\n#     ]\n# )\n\n# model = Pipeline(steps=[\n#     (\"preprocess\", preprocess),\n#     (\"clf\", RandomForestClassifier(\n#         n_estimators=600,\n#         max_depth=20,\n#         min_samples_split=2,\n#         min_samples_leaf=1,\n#         max_features=\"sqrt\",\n#         bootstrap=True,\n#         class_weight=\"balanced\",\n#         random_state=42\n#     ))\n# ])\n# # ================================\n# # 7. TRAIN/VALIDATION SPLIT\n# # ================================\n# from sklearn.model_selection import train_test_split\n# X_train, X_val, y_train, y_val = train_test_split(\n#     X, y_encoded,\n#     test_size=0.2,\n#     random_state=42,\n#     stratify=y_encoded\n# )\n\n# # ================================\n# # 8. TRAIN MODEL\n# # ================================\n# model.fit(X_train, y_train)\n\n# # ================================\n# # 9. EVALUATION\n# # ================================\n# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# y_pred = model.predict(X_val)\n# y_prob = model.predict_proba(X_val)\n\n# print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n# print(\"Precision:\", precision_score(y_val, y_pred, average='macro'))\n# print(\"Recall   :\", recall_score(y_val, y_pred, average='macro'))\n# print(\"F1 Score :\", f1_score(y_val, y_pred, average='macro'))\n# print(\"ROC AUC  :\", roc_auc_score(y_val, y_prob, multi_class='ovr'))\n\n# # ================================\n# # 10. TRAIN FULL MODEL\n# # ================================\n# model.fit(X, y_encoded)\n\n# # ================================\n# # 11. PREDICT TEST SET (PROBABILITIES)\n# # ================================\n# test_pred = model.predict_proba(test)\n\n# # ================================\n# # 12. FINAL SUBMISSION\n# # ================================\n# submission = pd.DataFrame({\n#     \"id\": test[\"id\"],\n#     \"Status_C\":  test_pred[:, le.transform(['C'])[0]],\n#     \"Status_CL\": test_pred[:, le.transform(['CL'])[0]],\n#     \"Status_D\":  test_pred[:, le.transform(['D'])[0]],\n# })\n\n# print(\"Duplicate IDs:\", submission[\"id\"].duplicated().sum())\n\n# submission.to_csv(\"submission2.csv\", index=False)\n# print(\"submission2.csv CREATED SUCCESSFULLY!\")\n# submission.head()\n\n_________________________________________________________________________\n\nðŸ”µ TYPE-1 : LABEL SUBMISSION\n(Metric: Accuracy / F1 etc.)\nSubmission: id,Status\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# ================= LOAD DATA =================\ntrain = pd.read_csv(\"train.csv\")\ntest  = pd.read_csv(\"test.csv\")\ntest_ids = test[\"id\"]\n\n# ================= DATA CLEANING =================\ntrain.fillna(train.median(numeric_only=True), inplace=True)\ntest.fillna(test.median(numeric_only=True), inplace=True)\n\nfor c in train.select_dtypes(include=\"object\"):\n    train[c].fillna(train[c].mode()[0], inplace=True)\n    if c in test.columns:\n        test[c].fillna(test[c].mode()[0], inplace=True)\n\n# ================= EDA =================\nsns.countplot(x=\"Status\", data=train); plt.title(\"Class Distribution\"); plt.show()\nsns.boxplot(data=train.select_dtypes(include=np.number)); plt.title(\"Outliers\"); plt.show()\ntrain.select_dtypes(include=np.number).hist(figsize=(10,6)); plt.show()\nsns.heatmap(train.select_dtypes(include=np.number).corr(), cmap=\"coolwarm\"); plt.show()\n\n# ================= FEATURES & TARGET =================\ny = train[\"Status\"]\nX = train.drop(columns=[\"Status\",\"id\"], errors=\"ignore\")\ntest = test.drop(columns=[\"id\"], errors=\"ignore\")\n\n# ================= ENCODING =================\ncat_cols = X.select_dtypes(include=\"object\").columns\noe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\nX[cat_cols] = oe.fit_transform(X[cat_cols])\ntest[cat_cols] = oe.transform(test[cat_cols])\n\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\n\n# ================= ALIGN & SCALE =================\nX, test = X.align(test, axis=1, fill_value=0)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest = scaler.transform(test)\n\n# ================= TRAIN / VALIDATION =================\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n)\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_tr, y_tr)\n\n# ================= EVALUATION =================\nval_pred = rf.predict(X_val)\n\nprint(\"Accuracy :\", accuracy_score(y_val, val_pred))\nprint(\"Precision:\", precision_score(y_val, val_pred, average=\"weighted\"))\nprint(\"Recall   :\", recall_score(y_val, val_pred, average=\"weighted\"))\nprint(\"F1 Score :\", f1_score(y_val, val_pred, average=\"weighted\"))\n\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, val_pred))\n\nsns.heatmap(confusion_matrix(y_val, val_pred),\n            annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n# ================= HYPERPARAMETER TUNING =================\ngrid = GridSearchCV(\n    rf, {\"n_estimators\":[100,200]}, cv=3, scoring=\"accuracy\"\n)\ngrid.fit(X_tr, y_tr)\n\n# ================= TEST PREDICTION =================\ntest_pred = grid.best_estimator_.predict(test)\ntest_pred = le.inverse_transform(test_pred)\n\n# ================= SUBMISSION =================\npd.DataFrame({\"id\":test_ids,\"Status\":test_pred}).to_csv(\n    \"submission_type1_labels.csv\", index=False\n)\n\nðŸ”´ TYPE-2 : PROBABILITY SUBMISSION\n(Metric: Multiclass Log Loss)\nSubmission: id,Status_C,Status_CL,Status_D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# ================= LOAD DATA =================\ntrain = pd.read_csv(\"train.csv\")\ntest  = pd.read_csv(\"test.csv\")\ntest_ids = test[\"id\"]\n\n# ================= CLEANING =================\ntrain.fillna(train.median(numeric_only=True), inplace=True)\ntest.fillna(test.median(numeric_only=True), inplace=True)\n\nfor c in train.select_dtypes(include=\"object\"):\n    train[c].fillna(train[c].mode()[0], inplace=True)\n    if c in test.columns:\n        test[c].fillna(test[c].mode()[0], inplace=True)\n\n# ================= EDA =================\nsns.countplot(x=\"Status\", data=train); plt.show()\ntrain[\"Status\"].value_counts().plot.pie(autopct=\"%1.1f%%\"); plt.show()\nsns.boxplot(data=train.select_dtypes(include=np.number)); plt.show()\nsns.heatmap(train.select_dtypes(include=np.number).corr(), cmap=\"coolwarm\"); plt.show()\n\n# ================= FEATURES & TARGET =================\ny = train[\"Status\"]\nX = train.drop(columns=[\"Status\",\"id\"], errors=\"ignore\")\ntest = test.drop(columns=[\"id\"], errors=\"ignore\")\n\n# ================= ENCODING =================\ncat_cols = X.select_dtypes(include=\"object\").columns\noe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\nX[cat_cols] = oe.fit_transform(X[cat_cols])\ntest[cat_cols] = oe.transform(test[cat_cols])\n\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\n\n# ================= ALIGN & SCALE =================\nX, test = X.align(test, axis=1, fill_value=0)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest = scaler.transform(test)\n\n# ================= TRAIN =================\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n)\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_tr, y_tr)\n\n# ================= EVALUATION =================\nval_pred = rf.predict(X_val)\n\nprint(\"Accuracy :\", accuracy_score(y_val, val_pred))\nprint(\"Precision:\", precision_score(y_val, val_pred, average=\"weighted\"))\nprint(\"Recall   :\", recall_score(y_val, val_pred, average=\"weighted\"))\nprint(\"F1 Score :\", f1_score(y_val, val_pred, average=\"weighted\"))\n\nsns.heatmap(confusion_matrix(y_val, val_pred),\n            annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.show()\n\n# ================= TUNING =================\ngrid = GridSearchCV(\n    rf, {\"n_estimators\":[100,200]}, cv=3\n)\ngrid.fit(X_tr, y_tr)\n\n# ================= PROBABILITY OUTPUT =================\nproba = grid.best_estimator_.predict_proba(test)\n\nsubmission = pd.DataFrame(proba, columns=le.classes_)\nsubmission.insert(0,\"id\",test_ids)\nsubmission.to_csv(\"submission_type2_probabilities.csv\", index=False)\n\nðŸŸ¢ TYPE-3 : ONE-HOT SUBMISSION\nSubmission: id,Status_C,Status_CL,Status_D (0/1)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load\ntrain = pd.read_csv(\"train.csv\")\ntest  = pd.read_csv(\"test.csv\")\ntest_ids = test[\"id\"]\n\n# Cleaning\ntrain.fillna(train.median(numeric_only=True), inplace=True)\ntest.fillna(test.median(numeric_only=True), inplace=True)\n\n# Target & features\ny = train[\"Status\"]\nX = train.drop(columns=[\"Status\",\"id\"], errors=\"ignore\")\ntest = test.drop(columns=[\"id\"], errors=\"ignore\")\n\n# Encoding\ncat_cols = X.select_dtypes(include=\"object\").columns\noe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\nX[cat_cols] = oe.fit_transform(X[cat_cols])\ntest[cat_cols] = oe.transform(test[cat_cols])\n\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\n\n# Align & scale\nX, test = X.align(test, axis=1, fill_value=0)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest = scaler.transform(test)\n\n# Train\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(X, y_enc)\n\n# One-hot output\nproba = rf.predict_proba(test)\none_hot = (proba == proba.max(axis=1, keepdims=True)).astype(int)\n\nsubmission = pd.DataFrame(one_hot, columns=le.classes_)\nsubmission.insert(0,\"id\",test_ids)\nsubmission.to_csv(\"submission_type3_onehot.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}